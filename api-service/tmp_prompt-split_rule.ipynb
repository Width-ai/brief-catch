{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.notion.so/scalr/bb14f178490140e0bac9016a30949b84?v=5cd436e7aea34a5daaa677ba63790450&p=4d815b136f164c429bc319f45b9db015&pm=s\n",
    "\n",
    "# [Rule Modifier] Rule splitting\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "- current UI can modify elements in a rule (e.g.),\n",
    "- need: ability to modify the rule as a whole\n",
    "\n",
    "## Proposed work\n",
    "\n",
    "Create a new prompt:\n",
    "\n",
    "- split rule based on the following two scenarios:\n",
    "\n",
    "1. [<or> Tags](https://www.notion.so/or-Tags-83b18d9960614627b7a3e5430193689e?pvs=21)\n",
    "2. [Rule too broad](https://www.notion.so/Rule-too-broad-26c11a6d02cc44c388710ce7d1a47915?pvs=21)\n",
    "\n",
    "The resulting split rules would need:\n",
    "\n",
    "- New IDs\n",
    "  - `\"BRIEFCATCH_{rand_int(40)}\"`\n",
    "- Rule `name` tag modified like so\n",
    "  - `\"BRIEFCATCH_PUNCHINESS_288.1\"`\n",
    "  - `\"BRIEFCATCH_PUNCHINESS_288.2\"`\n",
    "\n",
    "### when deploying\n",
    "\n",
    "- Use the rule modification checker (developed in previous story) to ensure example / suggestion tags match the pattern after splitting the rules\n",
    "\n",
    "# Success criteria\n",
    "\n",
    "The criteria that must be met in order to consider this project a success.\n",
    "\n",
    "- UI updated to allow for the splitting of rules\n",
    "  - \"Separate branch of logic\"\n",
    "    - not sure what is meant here\n",
    "  - Split option added to drop down\n",
    "  - Once user selects “Split”, i think a secondary select to determine why the user wants to split the rule would be necessary\n",
    "    - `<or> tag` or `rule too broad` could be the options\n",
    "      - If there is no <or> tag present in the rule we could drop that option\n",
    "    - <or> tag splitting won’t need other user input or need to call GPT to split the rule up\n",
    "    - if the rule is too broad, then we will need a field for user input and call GPT to identify how to best split the rules\n",
    "- Splitting a Rule prompt written and tested\n",
    "- Method for creating PR for updating repo will need to change too\n",
    "  - Instead of replacing a rule that has been modified or just adding in new rules, we will need to delete the original rule that has now been split into N many\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this rule as an example\n",
    "\n",
    "```\n",
    "<rule id=\"BRIEFCATCH_11012406027615556274904173201077833804\" name=\"BRIEFCATCH_PUNCHINESS_288\">\n",
    "    <pattern>\n",
    "        <or>\n",
    "            <token inflected=\"yes\">inquire</token>\n",
    "            <token>inquiry</token>\n",
    "        </or>\n",
    "        <token>as</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would direct language...</message>\n",
    "    <suggestion>\\1 into</suggestion>\n",
    "    <suggestion>\\1 about</suggestion>\n",
    "    <suggestion>\\1 in</suggestion>\n",
    "    <suggestion>\\1 from</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":4,\"priority\":\"3.209\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"inquiry into|inquiry about|inquiry in|inquiry from\">The <marker>inquiry as to</marker> the strong majority of documents and testimony sought continues.</example>\n",
    "</rule>\n",
    "```\n",
    "\n",
    "We would need to split the rule into two rules:\n",
    "\n",
    "`Rule 1` - just the inflected token for `inquire` (this one requires an update to the example tag)\n",
    "\n",
    "```\n",
    "<rule id=\"BRIEFCATCH_11012406027615556274904173201077833804\" name=\"BRIEFCATCH_PUNCHINESS_288.1\">\n",
    "    <pattern>\n",
    "        <token inflected=\"yes\">inquire</token>\n",
    "        <token>as</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would direct language...</message>\n",
    "    <suggestion>\\1 into</suggestion>\n",
    "    <suggestion>\\1 about</suggestion>\n",
    "    <suggestion>\\1 in</suggestion>\n",
    "    <suggestion>\\1 from</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":4,\"priority\":\"3.209\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"inquiry into|inquiry about|inquiry in|inquiry from\">He <marker>inquired as to</marker> the willpower of the group</example>\n",
    "</rule>\n",
    "```\n",
    "\n",
    "`Rule 2` - just the token for `inquiry` (no change for the example tag)\n",
    "\n",
    "```\n",
    "<rule id=\"BRIEFCATCH_11012406027615556274904173201077833804\" name=\"BRIEFCATCH_PUNCHINESS_288.2\">\n",
    "    <pattern>\n",
    "        <token>inquiry</token>\n",
    "        <token>as</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would direct language...</message>\n",
    "    <suggestion>\\1 into</suggestion>\n",
    "    <suggestion>\\1 about</suggestion>\n",
    "    <suggestion>\\1 in</suggestion>\n",
    "    <suggestion>\\1 from</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":4,\"priority\":\"3.209\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"inquiry into|inquiry about|inquiry in|inquiry from\">The <marker>inquiry as to</marker> the strong majority of documents and testimony sought continues.</example>\n",
    "</rule>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this rule as an example\n",
    "\n",
    "original_rule = \"\"\"\n",
    "<rule id=\"BRIEFCATCH_11012406027615556274904173201077833804\" name=\"BRIEFCATCH_PUNCHINESS_288\">\n",
    "    <pattern>\n",
    "        <or>\n",
    "            <token inflected=\"yes\">inquire</token>\n",
    "            <token>inquiry</token>\n",
    "        </or>\n",
    "        <token>as</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would direct language...</message>\n",
    "    <suggestion>\\1 into</suggestion>\n",
    "    <suggestion>\\1 about</suggestion>\n",
    "    <suggestion>\\1 in</suggestion>\n",
    "    <suggestion>\\1 from</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":4,\"priority\":\"3.209\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"inquiry into|inquiry about|inquiry in|inquiry from\">The <marker>inquiry as to</marker> the strong majority of documents and testimony sought continues.</example>\n",
    "</rule>\n",
    "\"\"\"\n",
    "\n",
    "# We would need to split the rule into two rules:\n",
    "\n",
    "# `Rule 1` - just the inflected token for `inquire` (this one requires an update to the example tag)\n",
    "\n",
    "rule_1 = \"\"\"\n",
    "<rule id=\"BRIEFCATCH_11012406027615556274904173201077833804\" name=\"BRIEFCATCH_PUNCHINESS_288.1\">\n",
    "    <pattern>\n",
    "        <token inflected=\"yes\">inquire</token>\n",
    "        <token>as</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would direct language...</message>\n",
    "    <suggestion>\\1 into</suggestion>\n",
    "    <suggestion>\\1 about</suggestion>\n",
    "    <suggestion>\\1 in</suggestion>\n",
    "    <suggestion>\\1 from</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":4,\"priority\":\"3.209\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"inquiry into|inquiry about|inquiry in|inquiry from\">He <marker>inquired as to</marker> the willpower of the group</example>\n",
    "</rule>\n",
    "\"\"\"\n",
    "\n",
    "# `Rule 2` - just the token for `inquiry` (no change for the example tag)\n",
    "\n",
    "rule_2 = \"\"\"\n",
    "<rule id=\"BRIEFCATCH_11012406027615556274904173201077833804\" name=\"BRIEFCATCH_PUNCHINESS_288.2\">\n",
    "    <pattern>\n",
    "        <token>inquiry</token>\n",
    "        <token>as</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would direct language...</message>\n",
    "    <suggestion>\\1 into</suggestion>\n",
    "    <suggestion>\\1 about</suggestion>\n",
    "    <suggestion>\\1 in</suggestion>\n",
    "    <suggestion>\\1 from</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":4,\"priority\":\"3.209\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"inquiry into|inquiry about|inquiry in|inquiry from\">The <marker>inquiry as to</marker> the strong majority of documents and testimony sought continues.</example>\n",
    "</rule>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split on or operands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rule = \"\"\"\n",
    "<rule id=\"BRIEFCATCH_331448315792705843437979608685430062094\" name=\"BRIEFCATCH_PUNCHINESS_1872\">\n",
    "    <antipattern>\n",
    "        <token postag=\"RB.*\" postag_regexp=\"yes\"/>\n",
    "        <token inflected=\"yes\">file</token>\n",
    "        <token min=\"0\"/>\n",
    "        <token regexp=\"yes\">motion|motions</token>\n",
    "        <token min=\"0\">seeking</token>\n",
    "        <token>to</token>\n",
    "    </antipattern>\n",
    "    <pattern>\n",
    "        <token inflected=\"yes\">file<exception>filing</exception></token>\n",
    "        <or>\n",
    "            <token min=\"0\" postag=\"PRP$\"/>\n",
    "            <token>a</token>\n",
    "        </or>\n",
    "        <token regexp=\"yes\">motion|motions</token>\n",
    "        <token min=\"0\">seeking</token>\n",
    "        <token>to</token>\n",
    "    </pattern>\n",
    "    <message>Would a stronger verb help engage the reader?|**Example** from Justice Kagan: \"Lange **moved to suppress** all evidence obtained after the officer entered his garage[.]\"|**Example** from Justice Kavanaugh: \"Before trial, Edwards **moved to suppress** the videotaped confession on the ground that the confession was involuntary.\"|**Example** from Morgan Chu: \"The defendants also **moved to transfer** another state court action to the state court considering the petitions.\"</message>\n",
    "    <suggestion><match no=\"1\" postag=\"(V.*)\" postag_regexp=\"yes\" postag_replace=\"$1\">move</match> to</suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":1,\"priority\":\"5.319\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"moved to\">The prosecution <marker>filed a motion seeking to</marker> have the victim.</example>\n",
    "    <example>It was a properly filed motion seeking to overturn the election.</example>\n",
    "</rule>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_rule = \"\"\"\n",
    "<rule id=\"BRIEFCATCH_245927502998399442504807079542143713153\" name=\"BRIEFCATCH_CONCISENESS_3959\">\n",
    "    <pattern>\n",
    "        <token inflected=\"yes\">assist</token>\n",
    "        <or>\n",
    "            <token min=\"0\" postag=\"JJ.*|PRP\" postag_regexp=\"yes\"/>\n",
    "            <token>the</token>\n",
    "        </or>\n",
    "        <or>\n",
    "            <token min=\"0\" postag=\"JJ.*|PRP|PRP\\$\" postag_regexp=\"yes\"/>\n",
    "            <token>the</token>\n",
    "        </or>\n",
    "        <token postag=\"N.*|PRP\" postag_regexp=\"yes\">\n",
    "            <exception>all</exception>\n",
    "            <exception>are</exception>\n",
    "            <exception>being</exception>\n",
    "            <exception>beliefs</exception>\n",
    "            <exception>but</exception>\n",
    "            <exception>by</exception>\n",
    "            <exception>can</exception>\n",
    "            <exception>circuit</exception>\n",
    "            <exception>clear</exception>\n",
    "            <exception>concerning</exception>\n",
    "            <exception>concerns</exception>\n",
    "            <exception>dissent</exception>\n",
    "            <exception>does</exception>\n",
    "            <exception>due</exception>\n",
    "            <exception>even</exception>\n",
    "            <exception>fails</exception>\n",
    "            <exception>find</exception>\n",
    "            <exception>finds</exception>\n",
    "            <exception>get</exception>\n",
    "            <exception>given</exception>\n",
    "            <exception>having</exception>\n",
    "            <exception>his</exception>\n",
    "            <exception>hold</exception>\n",
    "            <exception>holds</exception>\n",
    "            <exception>if</exception>\n",
    "            <exception>in</exception>\n",
    "            <exception>left</exception>\n",
    "            <exception>like</exception>\n",
    "            <exception>likes</exception>\n",
    "            <exception>long</exception>\n",
    "            <exception>make</exception>\n",
    "            <exception>makes</exception>\n",
    "            <exception>may</exception>\n",
    "            <exception>might</exception>\n",
    "            <exception>must</exception>\n",
    "            <exception>no</exception>\n",
    "            <exception>note</exception>\n",
    "            <exception>one</exception>\n",
    "            <exception>or</exception>\n",
    "            <exception>other</exception>\n",
    "            <exception>prior</exception>\n",
    "            <exception>regarding</exception>\n",
    "            <exception>see</exception>\n",
    "            <exception>then</exception>\n",
    "            <exception>try</exception>\n",
    "            <exception>will</exception>\n",
    "        </token>\n",
    "        <token>in</token>\n",
    "        <token postag=\"VBG\">\n",
    "            <exception>regarding</exception>\n",
    "            <exception>concerning</exception>\n",
    "            <exception>pending</exception>\n",
    "            <exception>following</exception>\n",
    "            <exception>standing</exception>\n",
    "            <exception>helping</exception>\n",
    "            <exception>neighboring</exception>\n",
    "            <exception>neighbouring</exception>\n",
    "        </token>\n",
    "    </pattern>\n",
    "    <message>Would using fewer words help sharpen the point?|**Example** from Justice Sotomayor: “The Affordable Care Act did this by, among other things, providing tax credits to **help people buy** insurance and establishing online marketplaces where insurers could sell plans.”</message>\n",
    "    <suggestion><match no=\"1\" postag=\"(V.*)\" postag_regexp=\"yes\" postag_replace=\"$1\">help</match> \\2 \\3 \\4 <match no=\"6\" postag=\"V.*\" postag_regexp=\"yes\" postag_replace=\"VB\"/></suggestion>\n",
    "    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":1,\"priority\":\"6.3201\",\"WORD\":true,\"OUTLOOK\":true}</short>\n",
    "    <example correction=\"Help his brother find\"><marker>Assist his brother in finding</marker> an apartment.</example>\n",
    "</rule>\n",
    "\"\"\"\n",
    "# split_rule_by_or_operands(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<rule id=\"BRIEFCATCH_331448315792705843437979608685430062094\" name=\"BRIEFCATCH_PUNCHINESS_1872\">\\n    <antipattern>\\n        <token postag=\"RB.*\" postag_regexp=\"yes\"/>\\n        <token inflected=\"yes\">file</token>\\n        <token min=\"0\"/>\\n        <token regexp=\"yes\">motion|motions</token>\\n        <token min=\"0\">seeking</token>\\n        <token>to</token>\\n    </antipattern>\\n    <pattern>\\n        <token inflected=\"yes\">file<exception>filing</exception></token>\\n        <token min=\"0\" postag=\"PRP$\"/>\\n        <token regexp=\"yes\">motion|motions</token>\\n        <token min=\"0\">seeking</token>\\n        <token>to</token>\\n    </pattern>\\n    <message>Would a stronger verb help engage the reader?|**Example** from Justice Kagan: \"Lange **moved to suppress** all evidence obtained after the officer entered his garage[.]\"|**Example** from Justice Kavanaugh: \"Before trial, Edwards **moved to suppress** the videotaped confession on the ground that the confession was involuntary.\"|**Example** from Morgan Chu: \"The defendants also **moved to transfer** another state court action to the state court considering the petitions.\"</message>\\n    <suggestion><match no=\"1\" postag=\"(V.*)\" postag_regexp=\"yes\" postag_replace=\"$1\">move</match> to</suggestion>\\n    <short>{\"ruleGroup\":null,\"ruleGroupIdx\":0,\"isConsistency\":false,\"isStyle\":true,\"correctionCount\":1,\"priority\":\"5.319\",\"WORD\":true,\"OUTLOOK\":true}</short>\\n    <example correction=\"moved to\">The prosecution <marker>filed a motion seeking to</marker> have the victim.</example>\\n    <example>It was a properly filed motion seeking to overturn the election.</example>\\n</rule>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_or_tag(rule_xml: str) -> str:\n",
    "    or_contentL = re.search(r\"(<or>.*?</or>)\", rule_xml, re.DOTALL)\n",
    "    if not or_contentL:\n",
    "        return None\n",
    "    return or_contentL.group(1)\n",
    "\n",
    "\n",
    "def extract_operands(or_input_string: str) -> List[str]:\n",
    "    # regular expression to find <token> tags\n",
    "    token_pattern = r\"(<token.*?/>|<token.*?</token>)\"\n",
    "    # extract all <token> tags\n",
    "    return re.findall(token_pattern, or_input_string, re.DOTALL)\n",
    "\n",
    "\n",
    "def split_rule_by_or_operands(input_rule: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    TODO: currently does not handle case where rule has two or tags.\n",
    "\n",
    "    \"\"\"\n",
    "    or_content = extract_or_tag(input_rule)\n",
    "    if not or_content:\n",
    "        return input_rule\n",
    "    operand_list = extract_operands(or_content)\n",
    "\n",
    "    split_rule = input_rule.split(or_content)\n",
    "    operand_rules = []\n",
    "    for operand_str in operand_list:\n",
    "        operand_rule = f\"{split_rule[0]}{operand_str}{split_rule[1]}\"\n",
    "        operand_rules.append(operand_rule)\n",
    "    return operand_rules\n",
    "\n",
    "\n",
    "## split rule that is too broad\n",
    "split_rule_by_or_operands(split_rule_by_or_operands(simple_rule)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split rule that is too broad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm\n",
    "\n",
    "- input: `input_rule`\n",
    "  - assume `input_rule` $\\isin \\{broad\\_rules\\}$\n",
    "- assemble dynamic prompt conditioned on rule\n",
    "  - POS present in `input_rule`\n",
    "  - maybe regex, if present in `input_rule`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rule = product_rule\n",
    "user_considerations = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/consult/miniconda3/envs/briefcatch/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2024-02-02 16:45:36,649 [WARNING] Session not authenticated, check api_key and customer_id are valid\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utils.dynamic_prompting import get_pos_tag_dicts_from_rule, POS_MAPS\n",
    "from utils.utils import generate_simple_message, call_gpt\n",
    "from domain.dynamic_prompting.prompt_leggo import (\n",
    "    GENERAL_INSTRUCTIONS_PROMPT,\n",
    "    SPLITTING_FEWSHOT_PROMPT,\n",
    "    REGEX_INSTRUCTIONS_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG Verb, gerund/present participle: eating, jumping, believing\n",
      "VB Verb, base form: eat, jump, believe, be, have\n",
      "VBD Verb, past tense: ate, jumped, believed\n",
      "VBN Verb, past participle: eaten, jumped, believed\n",
      "VBP Verb, non-3rd ps. sing. present: eat, jump, believe, am (as in 'I am'), are\n",
      "VBZ Verb, 3rd ps. sing. present: eats, jumps, believes, is, has\n"
     ]
    }
   ],
   "source": [
    "# grab part of speech tag from rule\n",
    "pos_tags_input_rule = get_pos_tag_dicts_from_rule(input_rule, list(POS_MAPS.keys()))\n",
    "# NOTE: prompt has the following POStags, including them manually here\n",
    "pos_tags_in_prompt = {\n",
    "    \"VB\": \"VB Verb, base form: eat, jump, believe, be, have\",\n",
    "    \"VBD\": \"VBD Verb, past tense: ate, jumped, believed\",\n",
    "    \"VBG\": \"VBG Verb, gerund/present participle: eating, jumping, believing\",\n",
    "    \"VBN\": \"VBN Verb, past participle: eaten, jumped, believed\",\n",
    "    \"VBP\": \"VBP Verb, non-3rd ps. sing. present: eat, jump, believe, am (as in 'I am'), are\",\n",
    "    \"VBZ\": \"VBZ Verb, 3rd ps. sing. present: eats, jumps, believes, is, has\",\n",
    "}\n",
    "all_pos = {**pos_tags_input_rule, **pos_tags_in_prompt}\n",
    "_replace_pos = \"\\n\".join([f\"{v}\" for k, v in all_pos.items()])\n",
    "print(_replace_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dynamic_prompting import rule_has_regex\n",
    "\n",
    "if rule_has_regex(input_rule):\n",
    "    _replace_regex = REGEX_INSTRUCTIONS_PROMPT\n",
    "else:\n",
    "    _replace_regex = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_replace_general_instruction = GENERAL_INSTRUCTIONS_PROMPT.format(\n",
    "    part_of_speech=_replace_pos,\n",
    "    regex_rules=_replace_regex,\n",
    ")\n",
    "_replace_task_instruction = \"\"\"\n",
    "You are a language system used for modifying gramatical logic encoded as XML rules. \n",
    "The user will provide you with (i) a rule that is deemed too broad (ii) some additional considerations. \n",
    "Your task is to split (i) the rule that is too broad while taking into account (ii) the provided additional considerations. \n",
    "Below I will provide you with some additional context and at the bottom of this message is an example of a rule being split.\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_template = \"\"\"\n",
    "{task_instruction}\n",
    "\n",
    "{general_instructions}\n",
    "\n",
    "{splitting_fewshot}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = final_prompt_template.format(\n",
    "    task_instruction=_replace_task_instruction,\n",
    "    general_instructions=_replace_general_instruction,\n",
    "    splitting_fewshot=SPLITTING_FEWSHOT_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = json.dumps(\n",
    "    {\n",
    "        \"rule_deemed_too_broad\": input_rule,\n",
    "        \"additional_considerations\": user_considerations,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m load_dotenv()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcall_gpt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_simple_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-1106-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wd/width/brief-catch/api-service/utils/utils.py:68\u001b[0m, in \u001b[0;36mcall_gpt\u001b[0;34m(model, messages, temperature, max_length)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_gpt\u001b[39m(model: \u001b[38;5;28mstr\u001b[39m, messages: List, temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, max_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Dict]:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Generic function to call GPT4 with specified messages\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     usage \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m     }\n\u001b[1;32m     80\u001b[0m     usage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compute_cost(usage, model)\n",
      "File \u001b[0;32m~/miniconda3/envs/briefcatch/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/briefcatch/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[1;32m    141\u001b[0m         timeout,\n\u001b[1;32m    142\u001b[0m         stream,\n\u001b[1;32m    143\u001b[0m         headers,\n\u001b[1;32m    144\u001b[0m         request_timeout,\n\u001b[1;32m    145\u001b[0m         typed_api_type,\n\u001b[1;32m    146\u001b[0m         requestor,\n\u001b[1;32m    147\u001b[0m         url,\n\u001b[1;32m    148\u001b[0m         params,\n\u001b[0;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__prepare_create_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/briefcatch/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m MAX_TIMEOUT\n\u001b[0;32m--> 106\u001b[0m requestor \u001b[38;5;241m=\u001b[39m \u001b[43mapi_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIRequestor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    115\u001b[0m     deployment_id,\n\u001b[1;32m    116\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     params,\n\u001b[1;32m    125\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/briefcatch/lib/python3.9/site-packages/openai/api_requestor.py:138\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     organization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m api_base \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    140\u001b[0m         ApiType\u001b[38;5;241m.\u001b[39mfrom_str(api_type)\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m api_type\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m ApiType\u001b[38;5;241m.\u001b[39mfrom_str(openai\u001b[38;5;241m.\u001b[39mapi_type)\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_version \u001b[38;5;241m=\u001b[39m api_version \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_version\n",
      "File \u001b[0;32m~/miniconda3/envs/briefcatch/lib/python3.9/site-packages/openai/util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAuthenticationError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key provided. You can set your API key in code using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key = <API-KEY>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key_path = <PATH>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
     ]
    }
   ],
   "source": [
    "call_gpt(\n",
    "    messages=generate_simple_message(system_prompt, user_prompt),\n",
    "    model=\"gpt-4-1106-preview\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "briefcatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
